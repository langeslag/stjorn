{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddac0928",
   "metadata": {},
   "source": [
    "# _Stjórn_ Plaintext Corpus Generation\n",
    "\n",
    "HTML from [heimskringla.no](https://heimskringla.no/wiki/Stj%C3%B3rn) following [Unger 1862](https://archive.org/details/stjorngammelnors00unge/page/n5/mode/2up), who primarily followed AM 226."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8d58b5-8877-4964-be5d-62a4b928dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re,json,copy\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "Path(\"raw\").mkdir(exist_ok=True)\n",
    "Path(\"clean\").mkdir(exist_ok=True)\n",
    "Path(\"plaintext\").mkdir(exist_ok=True)\n",
    "Path(\"nlp\").mkdir(exist_ok=True)\n",
    "Path(\"split/commentary\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"split/bible\").mkdir(exist_ok=True)\n",
    "Path(\"split/unmarked\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7607ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = {\n",
    "        'prologue': 'https://heimskringla.no/wiki/Prolog_(Stj%C3%B3rn)',\n",
    "        'introduction': 'https://heimskringla.no/wiki/Indledning_(Stj%C3%B3rn)',\n",
    "        'gn': 'https://heimskringla.no/wiki/I._Mosebog',\n",
    "        'ex': 'https://heimskringla.no/wiki/II._Mosebog',\n",
    "        'lv': 'https://heimskringla.no/wiki/III._Mosebog',\n",
    "        'nm': 'https://heimskringla.no/wiki/IV._Mosebog',\n",
    "        'dt': 'https://heimskringla.no/wiki/V._Mosebog',\n",
    "        'ios': 'https://heimskringla.no/wiki/Josv%C3%A6_Bog',\n",
    "        'idc': 'https://heimskringla.no/wiki/Dommernes_Bog',\n",
    "        'rt': 'https://heimskringla.no/wiki/Ruths_Bog',\n",
    "        '1sm': 'https://heimskringla.no/wiki/I._Samuels_Bog',\n",
    "        '2sm': 'https://heimskringla.no/wiki/II._Samuels_Bog',\n",
    "        '3rg': 'https://heimskringla.no/wiki/I._Kongernes_Bog',\n",
    "        '4rg': 'https://heimskringla.no/wiki/II._Kongernes_Bog'\n",
    "        }\n",
    "\n",
    "path = 'raw/'\n",
    "for title, url in remote.items():\n",
    "    local = os.path.join(path, title) + '.html'\n",
    "    if not(os.path.exists(local) and os.path.getsize(local) > 0):\n",
    "        urlretrieve(url, local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75bdd40-4503-49ab-8a72-026937be0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: normalize remaining long vowels?\n",
    "\n",
    "documents = ['prologue', 'introduction', 'gn', 'ex', 'lv', 'nm', 'dt', 'ios', 'idc', 'rt', '1sm', '2sm', '3rg', '4rg']\n",
    "\n",
    "def normalize(target):\n",
    "    matrix = {\n",
    "        'j': 'i',\n",
    "        'v': 'u',\n",
    "        'ę': 'æ',\n",
    "        'ẻ': 'æ',\n",
    "        'ỏ': 'ǫ',\n",
    "        'đ': 'ð',\n",
    "        'afþeirri': 'af þeirri' # an error in the HTML of 1 Samuel\n",
    "    }\n",
    "    for k,v in matrix.items():\n",
    "        target = target.replace(k, v)\n",
    "    return target\n",
    "\n",
    "with open('rubrics.json') as json_data:\n",
    "    rubrics = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41401aa-2818-4229-b437-2051e751acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in documents:\n",
    "    raw_html = 'raw/' + book + '.html'\n",
    "    clean_html = 'clean/' + book + '.html'\n",
    "    text_file = 'plaintext/' + book + '.txt'\n",
    "    nlp_file = 'nlp/' + book + '.txt'\n",
    "    commentary_file = 'split/commentary/' + book + '.txt'\n",
    "    bible_file = 'split/bible/' + book + '.txt'\n",
    "    unmarked_file = 'split/unmarked/' + book + '.txt'\n",
    "    commentary = []\n",
    "    bible = []\n",
    "    unmarked = []\n",
    "    with open(raw_html) as html_doc:\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # Since heimskringla.no lacks a class name for the main text that we could select for,\n",
    "        # we'll just nuke all the unwanted nodes instead:\n",
    "        unwanted_elements = ['title', 'script', 'meta', 'link', 'center', 'a', 'sup', 'table']\n",
    "        unwanted_classes = ['.mw-references-wrap', '.printfooter', '.catlinks', '.visualClear', '.mw-indicators mw-body-content', '.toccolours', '.thumb tright', '.thumbinner', '.thumbcaption', '.magnify'] # spaced entries aren't caught\n",
    "        unwanted_ids = ['mw-page-base', 'mw-head-base', 'mw-navigation', 'toc', 'siteSub', 'contentSub', 'jump-to-nav', 'mw-parser-output', 'firstHeading', 'footer', 'footer-info-lastmod']\n",
    "        #unwanted_attribute_elements = ['div, span']\n",
    "        for element in unwanted_elements:\n",
    "            for match in soup.find_all(element):\n",
    "                match.decompose()\n",
    "        for attr_class in unwanted_classes:\n",
    "            for match in soup.css.select(attr_class):\n",
    "                match.decompose()\n",
    "        for match in soup.select('div[class^=toclimit]'):\n",
    "            match.decompose()\n",
    "        for match in soup.select('div[class^=thumb]'):\n",
    "            match.decompose()\n",
    "        for match in soup.select('div[class^=mw-indicators]'):\n",
    "            match.decompose()\n",
    "        for identifier in unwanted_ids:\n",
    "            for match in soup.find_all(id=identifier):\n",
    "                match.decompose()\n",
    "        for match in soup.find_all(\"b\", string=\"Fotnoter\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"b\", string=\"Fotnoter:\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"i\", string=\"Innskudd fra Andre Krønikebok, kapittel 20.\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"p\", string=\"()\"):\n",
    "            match.decompose()\n",
    "        for match in soup.find_all(\"i\"):\n",
    "            match.insert(0, '{')\n",
    "            match.append('}')\n",
    "        all_divs = soup.find_all('div')\n",
    "        for div in all_divs:\n",
    "            for element in div(string=lambda string: isinstance(string, Comment)):\n",
    "                element.extract()\n",
    "        soup.head.clear()\n",
    "        with open(clean_html, 'w') as file:\n",
    "            file.write(str(soup))\n",
    "        # Here is where I delete rubrics and chapter numbers\n",
    "        # to leave only the text for NLP evaluation.\n",
    "        # Comment out span to reintroduce chapter headings!\n",
    "        last_unwanted_elements = ['b', 'span']\n",
    "        for element in last_unwanted_elements:\n",
    "            for match in soup.find_all(element):\n",
    "                match.decompose()\n",
    "        with open(text_file, 'w') as file:\n",
    "            file.write(soup.get_text())\n",
    "        # Now reopen as plaintext to remove punctuation and blank lines:\n",
    "        unwanted_chars = ['.', ':', ';', '?', '!', '[', ']', '(', ')']\n",
    "        plaintext = open(text_file).readlines()\n",
    "        flattened = []\n",
    "        for line in plaintext:\n",
    "            for character in unwanted_chars:\n",
    "                line = line.replace(character, '')\n",
    "            line = line.replace('{', '[').replace('}', ']')\n",
    "            line = normalize(' '.join(line.lower().lstrip().split()))\n",
    "            if not re.match('^$', line):\n",
    "                flattened.append(line + '\\n')\n",
    "        with open(nlp_file, 'w') as f:\n",
    "            f.writelines(flattened)\n",
    "        # Now further process the plaintext to split the content into Bible, commentary, unmarked\n",
    "        # (only Gn and Ex have such references):\n",
    "        if book in ['gn', 'ex']:\n",
    "            split_document = '\\n'.join(flattened)\n",
    "            split_document = re.sub('\\[', '\\n[', split_document)\n",
    "            split_lines = split_document.split('\\n')\n",
    "            for line in split_lines:\n",
    "                if not re.match('^$', line):\n",
    "                    source_ref = re.match('\\[([^]]*)\\]', line)\n",
    "                    if source_ref:\n",
    "                        reference = source_ref.group(0).strip('[]')\n",
    "                        if reference in rubrics['bible']:\n",
    "                            bible.append(line.replace(']', ':').strip('[') + '\\n')\n",
    "                        elif reference in rubrics['commentary']:\n",
    "                            commentary.append(line.replace(']', ':').strip('\\[') + '\\n')\n",
    "                    else:\n",
    "                        unmarked.append(line + '\\n')\n",
    "            with open(bible_file, 'w') as f:\n",
    "                f.writelines(bible)\n",
    "            with open(commentary_file, 'w') as f:\n",
    "                f.writelines(commentary)\n",
    "            with open(unmarked_file, 'w') as f:\n",
    "                f.writelines(unmarked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
